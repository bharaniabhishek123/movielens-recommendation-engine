{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalToZero(tf.keras.constraints.Constraint):\n",
    "    def __call__(self, w):\n",
    "        \"\"\"Set diagonal to zero\"\"\"\n",
    "        q = tf.linalg.set_diag(w, tf.zeros(w.shape[0:-1]), name=None)\n",
    "        return q\n",
    "\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a basket.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim), stddev=1.)\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class VASP(Model):\n",
    "    class Model(tf.keras.Model):\n",
    "        def __init__(self, num_words, latent=1024, hidden=1024, items_sampling=1.):\n",
    "            \"\"\"\n",
    "            num_words             nr of items in dataset (size of tokenizer)\n",
    "            latent                size of latent space\n",
    "            hidden                size of hidden layers\n",
    "            items_sampling        Large items datatsets can be very gpu memory consuming in EASE layer.\n",
    "                                  This coefficient reduces number of ease parametrs by taking only\n",
    "                                  fraction of items sorted by popularity as input for model.\n",
    "                                  Note: This coef should be somewhere around coverage@100 achieved by full\n",
    "                                  size model.\n",
    "                                  For ML20M this coef should be between 0.4888 (coverage@100 for full model)\n",
    "                                  and 1.0\n",
    "                                  For Netflix this coef should be between 0.7055 (coverage@100 for full\n",
    "                                  model) and 1.0\n",
    "            \"\"\"\n",
    "            super(VASP.Model, self).__init__()\n",
    "\n",
    "            self.sampled_items = int(num_words * items_sampling)\n",
    "\n",
    "            assert self.sampled_items > 0\n",
    "            assert self.sampled_items <= num_words\n",
    "\n",
    "            self.s = self.sampled_items < num_words\n",
    "\n",
    "            # ************* ENCODER ***********************\n",
    "            self.encoder1 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln1 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder2 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln2 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder3 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln3 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder4 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln4 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder5 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln5 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder6 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln6 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder7 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln7 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "            # ************* SAMPLING **********************\n",
    "            self.dense_mean = tf.keras.layers.Dense(latent,\n",
    "                                                    name=\"Mean\")\n",
    "            self.dense_log_var = tf.keras.layers.Dense(latent,\n",
    "                                                       name=\"log_var\")\n",
    "\n",
    "            self.sampling = Sampling(name='Sampler')\n",
    "\n",
    "            # ************* DECODER ***********************\n",
    "            self.decoder1 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln1 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder2 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln2 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder3 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln3 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder4 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln4 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder5 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln5 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "            self.decoder_resnet = tf.keras.layers.Dense(self.sampled_items,\n",
    "                                                        activation='sigmoid',\n",
    "                                                        name=\"DecoderR\")\n",
    "            self.decoder_latent = tf.keras.layers.Dense(self.sampled_items,\n",
    "                                                        activation='sigmoid',\n",
    "                                                        name=\"DecoderL\")\n",
    "\n",
    "            # ************* PARALLEL SHALLOW PATH *********\n",
    "\n",
    "            self.ease = tf.keras.layers.Dense(\n",
    "                self.sampled_items,\n",
    "                activation='sigmoid',\n",
    "                use_bias=False,\n",
    "                kernel_constraint=DiagonalToZero(),  # critical to prevent learning simple identity\n",
    "            )\n",
    "\n",
    "        def call(self, x, training=None):\n",
    "            sampling = self.s\n",
    "            if sampling:\n",
    "                sampled_x = x[:, :self.sampled_items]\n",
    "                non_sampled = x[:, self.sampled_items:] * 0.\n",
    "            else:\n",
    "                sampled_x = x\n",
    "\n",
    "            z_mean, z_log_var, z = self.encode(sampled_x)\n",
    "            if training:\n",
    "                d = self.decode(z)\n",
    "                # Add KL divergence regularization loss.\n",
    "                kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "                kl_loss = tf.reduce_mean(kl_loss)\n",
    "                kl_loss *= -0.5\n",
    "                self.add_loss(kl_loss)\n",
    "                self.add_metric(kl_loss, name=\"kl_div\")\n",
    "            else:\n",
    "                d = self.decode(z_mean)\n",
    "\n",
    "            if sampling:\n",
    "                d = tf.concat([d, non_sampled], axis=-1)\n",
    "\n",
    "            ease = self.ease(sampled_x)\n",
    "\n",
    "            if sampling:\n",
    "                ease = tf.concat([ease, non_sampled], axis=-1)\n",
    "\n",
    "            return d * ease\n",
    "\n",
    "        def decode(self, x):\n",
    "            e0 = x\n",
    "            e1 = self.dln1(tf.keras.activations.swish(self.decoder1(e0)))\n",
    "            e2 = self.dln2(tf.keras.activations.swish(self.decoder2(e1) + e1))\n",
    "            e3 = self.dln3(tf.keras.activations.swish(self.decoder3(e2) + e1 + e2))\n",
    "            e4 = self.dln4(tf.keras.activations.swish(self.decoder4(e3) + e1 + e2 + e3))\n",
    "            e5 = self.dln5(tf.keras.activations.swish(self.decoder5(e4) + e1 + e2 + e3 + e4))\n",
    "\n",
    "            dr = self.decoder_resnet(e5)\n",
    "            dl = self.decoder_latent(x)\n",
    "\n",
    "            return dr * dl\n",
    "\n",
    "        def encode(self, x):\n",
    "            e0 = x\n",
    "            e1 = self.ln1(tf.keras.activations.swish(self.encoder1(e0)))\n",
    "            e2 = self.ln2(tf.keras.activations.swish(self.encoder2(e1) + e1))\n",
    "            e3 = self.ln3(tf.keras.activations.swish(self.encoder3(e2) + e1 + e2))\n",
    "            e4 = self.ln4(tf.keras.activations.swish(self.encoder4(e3) + e1 + e2 + e3))\n",
    "            e5 = self.ln5(tf.keras.activations.swish(self.encoder5(e4) + e1 + e2 + e3 + e4))\n",
    "            e6 = self.ln6(tf.keras.activations.swish(self.encoder6(e5) + e1 + e2 + e3 + e4 + e5))\n",
    "            e7 = self.ln7(tf.keras.activations.swish(self.encoder7(e6) + e1 + e2 + e3 + e4 + e5 + e6))\n",
    "\n",
    "            z_mean = self.dense_mean(e7)\n",
    "            z_log_var = self.dense_log_var(e7)\n",
    "            z = self.sampling((z_mean, z_log_var))\n",
    "\n",
    "            return z_mean, z_log_var, z\n",
    "\n",
    "    def create_model(self, latent=2048, hidden=4096, ease_items_sampling=1., summary=False):\n",
    "        self.model = VASP.Model(self.dataset.num_words, latent, hidden, ease_items_sampling)\n",
    "        \n",
    "        self.model(self.split.train_gen[0][0])\n",
    "\n",
    "        if summary:\n",
    "            self.model.summary()\n",
    "        self.mc = MetricsCallback(self)\n",
    "\n",
    "    def compile_model(self, lr=0.00002, fl_alpha=0.25, fl_gamma=2.0):\n",
    "        \"\"\"\n",
    "        lr         learning rate of Nadam optimizer\n",
    "        fl_alpha   alpha parameter of focal crossentropy\n",
    "        fl_gamma   gamma parameter of focal crossentropy\n",
    "        \"\"\"\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Nadam(lr),\n",
    "            loss=lambda x, y: tfa.losses.sigmoid_focal_crossentropy(x, y, alpha=fl_alpha, gamma=fl_gamma),\n",
    "            metrics=['mse', cosine_loss]\n",
    "        )\n",
    "\n",
    "    def train_model(self, epochs=150):\n",
    "        self.model.fit(\n",
    "            self.split.train_gen,\n",
    "            validation_data=self.split.validation_gen,\n",
    "            epochs=epochs,\n",
    "            callbacks=[self.mc]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading users_pu5\n",
      "Reading items_pu5\n",
      "Reading purchases_txt_pu5\n",
      "Reading items_sorted_pu5\n",
      "Reading users_sorted_pu5\n",
      "Read all in 1.3948488235473633\n",
      "Tokenizer trained for 20721 items.\n",
      "Creating 1 splits of 10000 samples each.\n",
      "Creating split nr. 1\n",
      "SplitGenerator init done in 9.945061922073364 secs.\n",
      "SplitGenerator init done in 0.8751280307769775 secs.\n",
      "SplitGenerator init done in 0.7490301132202148 secs.\n",
      "Creating evaluator\n",
      "Creating test split evaluator with leave_random_20_pct_out method.\n",
      "Creating validation split evaluator with leave_random_20_pct_out method.\n"
     ]
    }
   ],
   "source": [
    "dataset = Data(d='../data/ml-20m/processed/', pruning='u5')\n",
    "dataset.splits = []\n",
    "dataset.create_splits(1, 10000, shuffle=False, generators=False)\n",
    "dataset.split.train_users = pd.read_json(\"../data/ml-20m/processed/train_users.json\").userid.apply(str).to_frame()\n",
    "dataset.split.validation_users = pd.read_json(\"../data/ml-20m/processed/val_users.json\").userid.apply(str).to_frame()\n",
    "dataset.split.test_users = pd.read_json(\"../data/ml-20m/processed/test_users.json\").userid.apply(str).to_frame()\n",
    "dataset.split.generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = VASP(dataset.split, name=\"VASP_ML20_1\")\n",
    "m.create_model(latent=2048, hidden=4096, ease_items_sampling=0.33)\n",
    "m.model.summary()\n",
    "print(\"=\" * 80)\n",
    "print(\"Train for 50 epochs with lr 0.00005\")\n",
    "m.compile_model(lr=0.00005, fl_alpha=0.25, fl_gamma=2.0)\n",
    "m.train_model(50)\n",
    "print(\"=\" * 80)\n",
    "print(\"Than train for 20 epochs with lr 0.00001\")\n",
    "m.compile_model(lr=0.00001, fl_alpha=0.25, fl_gamma=2.0)\n",
    "m.train_model(20)\n",
    "print(\"=\" * 80)\n",
    "print(\"Than train for 20 epochs with lr 0.000001\")\n",
    "m.compile_model(lr=0.00001, fl_alpha=0.25, fl_gamma=2.0)\n",
    "m.train_model(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading users_pu5\n",
      "Reading items_pu5\n",
      "Reading purchases_txt_pu5\n",
      "Reading items_sorted_pu5\n",
      "Reading users_sorted_pu5\n",
      "Read all in 1.5181760787963867\n",
      "Tokenizer trained for 20721 items.\n",
      "Creating 1 splits of 2500 samples each.\n",
      "Creating split nr. 1\n",
      "SplitGenerator init done in 1.7124419212341309 secs.\n",
      "SplitGenerator init done in 0.1869361400604248 secs.\n",
      "SplitGenerator init done in 0.20406317710876465 secs.\n",
      "Creating evaluator\n",
      "Creating test split evaluator with leave_random_20_pct_out method.\n",
      "Creating validation split evaluator with leave_random_20_pct_out method.\n"
     ]
    }
   ],
   "source": [
    "small_dataset = Data(d='../data/ml-20m/small_processed/', pruning='u5')\n",
    "small_dataset.splits = []\n",
    "small_dataset.create_splits(1, 2500, shuffle=False, generators=False,batch_size=128)\n",
    "small_dataset.split.train_users = pd.read_json(\"../data/ml-20m/small_processed/few_train_users.json\").userid.apply(str).to_frame()\n",
    "small_dataset.split.validation_users = pd.read_json(\"../data/ml-20m/small_processed/few_val_users.json\").userid.apply(str).to_frame()\n",
    "small_dataset.split.test_users = pd.read_json(\"../data/ml-20m/small_processed/few_test_users.json\").userid.apply(str).to_frame()\n",
    "small_dataset.split.generators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/vasp.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/vasp.ipynb#ch0000011?line=0'>1</a>\u001b[0m small_dataset\u001b[39m.\u001b[39;49msplit\u001b[39m.\u001b[39;49mtrain_gen[\u001b[39m0\u001b[39;49m]\n",
      "File \u001b[0;32m~/PycharmProjects/movielens-recommendation-engine/VASP/utils.py:395\u001b[0m, in \u001b[0;36mSplitGenerator.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.py?line=392'>393</a>\u001b[0m index3 \u001b[39m=\u001b[39m indices[index \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m]\n\u001b[1;32m    <a href='file:///Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.py?line=393'>394</a>\u001b[0m index4 \u001b[39m=\u001b[39m indices[index \u001b[39m+\u001b[39m \u001b[39m3\u001b[39m]\n\u001b[0;32m--> <a href='file:///Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.py?line=394'>395</a>\u001b[0m index5 \u001b[39m=\u001b[39m indices[index \u001b[39m+\u001b[39;49m \u001b[39m4\u001b[39;49m]\n\u001b[1;32m    <a href='file:///Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.py?line=396'>397</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_data:\n\u001b[1;32m    <a href='file:///Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.py?line=397'>398</a>\u001b[0m     data_slice \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_np[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m*\u001b[39m index:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m*\u001b[39m index \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "small_dataset.split.train_gen[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = VASP(small_dataset.split, name=\"VASP_ML1M_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22335\n"
     ]
    }
   ],
   "source": [
    "print(m1.split.train_gen.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'utils.SplitGenerator'>\n",
      "20\n",
      "22335\n",
      "1024\n",
      "(2048, 20721)\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# print(small_dataset.split.train_gen[0][1].shape)\n",
    "# print(dataset.split.train_gen[0][1].shape)\n",
    "\n",
    "print(type(m1.split.train_gen))\n",
    "print(m1.split.train_gen.__len__())\n",
    "print(m1.split.train_gen.length)\n",
    "print(m1.split.train_gen.batch_size)\n",
    "print(m1.split.train_gen[0][0].shape)\n",
    "\n",
    "print(int(np.floor(m1.split.train_gen.length / m1.split.train_gen.batch_size)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  28008448  \n",
      "_________________________________________________________________\n",
      "layer_normalization (LayerNo multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_1 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_2 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_3 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_4 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_5 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_6 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "Mean (Dense)                 multiple                  8390656   \n",
      "_________________________________________________________________\n",
      "log_var (Dense)              multiple                  8390656   \n",
      "_________________________________________________________________\n",
      "Sampler (Sampling)           multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "layer_normalization_7 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_8 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_9 (Layer multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_10 (Laye multiple                  8192      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  16781312  \n",
      "_________________________________________________________________\n",
      "layer_normalization_11 (Laye multiple                  8192      \n",
      "_________________________________________________________________\n",
      "DecoderR (Dense)             multiple                  28011189  \n",
      "_________________________________________________________________\n",
      "DecoderL (Dense)             multiple                  14009013  \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  46744569  \n",
      "=================================================================\n",
      "Total params: 309,858,659\n",
      "Trainable params: 309,858,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "================================================================================\n",
      "Train for 50 epochs with lr 0.00005\n",
      " 2/20 [==>...........................] - ETA: 5:34 - loss: 47.1569 - mse: 0.0085 - cosine_loss: 0.9463 - kl_div: 1.1302"
     ]
    }
   ],
   "source": [
    "m1 = VASP(small_dataset.split, name=\"VASP_ML1M_1\")\n",
    "m1.create_model(latent=2048, hidden=4096, ease_items_sampling=0.33)\n",
    "m1.model.summary()\n",
    "print(\"=\" * 80)\n",
    "print(\"Train for 50 epochs with lr 0.00005\")\n",
    "m1.compile_model(lr=0.00005, fl_alpha=0.25, fl_gamma=2.0)\n",
    "m1.train_model(1)\n",
    "# print(\"=\" * 80)\n",
    "# print(\"Than train for 20 epochs with lr 0.00001\")\n",
    "# m1.compile_model(lr=0.00001, fl_alpha=0.25, fl_gamma=2.0)\n",
    "# m1.train_model(20)\n",
    "# print(\"=\" * 80)\n",
    "# print(\"Than train for 20 epochs with lr 0.000001\")\n",
    "# m1.compile_model(lr=0.00001, fl_alpha=0.25, fl_gamma=2.0)\n",
    "# m1.train_model(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3aa81f53feaf22594607225e353cd7b61927a2847d5a10e1d40f929f7c9ceb6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('airflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
