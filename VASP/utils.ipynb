{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import bottleneck as bn\n",
    "from random import randrange, shuffle\n",
    "import random\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "SEED = DEFAULT_SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "\n",
    "def set_seed(seed=DEFAULT_SEED):\n",
    "    \"\"\"\n",
    "    Set random seed in all used libs.\n",
    "    \"\"\"\n",
    "    global SEED\n",
    "    SEED = seed\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def get_seed():\n",
    "    return SEED\n",
    "\n",
    "\n",
    "def shufflestr(x):\n",
    "    \"\"\"\n",
    "    Shuffle randomly items in string separated by comma.\n",
    "    \"\"\"\n",
    "    p = x.split(',')\n",
    "    random.shuffle(p)\n",
    "    return \",\".join(p)\n",
    "\n",
    "\n",
    "def split2_50(x):\n",
    "    \"\"\"\n",
    "    Returns first half of items in string separated by comma.\n",
    "    \"\"\"\n",
    "    p = x.split(',')\n",
    "    s = int(len(p) * .5)\n",
    "    return \",\".join(p[:s])\n",
    "\n",
    "\n",
    "def split1_50(x):\n",
    "    \"\"\"\n",
    "    Returns second half of items in string separated by comma.\n",
    "    \"\"\"\n",
    "    p = x.split(',')\n",
    "    s = int(len(p) * .5)\n",
    "    return \",\".join(p[s:])\n",
    "\n",
    "\n",
    "def split75(x):\n",
    "    \"\"\"\n",
    "    Returns first three quarters of items in string separated by comma.\n",
    "    \"\"\"\n",
    "    p = x.split(',')\n",
    "    s = int(len(p) * .75)\n",
    "    return \",\".join(p[:s])\n",
    "\n",
    "\n",
    "def split25(x):\n",
    "    \"\"\"\n",
    "    Returns last quarter of items in string separated by comma.\n",
    "    \"\"\"\n",
    "    p = x.split(',')\n",
    "    s = int(len(p) * .75)\n",
    "    return \",\".join(p[s:])\n",
    "\n",
    "\n",
    "# TF functions\n",
    "\n",
    "@tf.function\n",
    "def cosmse(x, y):\n",
    "    # x=tf.cast(x,'float32')\n",
    "    # y=tf.cast(y,'float32')\n",
    "    a = tf.constant(ALPHA) * cosine_loss(x, y)\n",
    "    d = tf.constant(BETA) * tf.keras.losses.MSE(x, y)\n",
    "    return a + d\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def cosine_loss(x, y):\n",
    "    return tf.keras.losses.cosine_similarity(x, y) + tf.constant(1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset classes\n",
    "class Data:\n",
    "    \"\"\"\n",
    "    Basic dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d=str(), pruning=False):\n",
    "        self.directory = d\n",
    "        if pruning:\n",
    "            self.read_all(p=\"_p\" + pruning)\n",
    "        else:\n",
    "            self.read_all()\n",
    "        # array of all data splits\n",
    "        self.splits = []\n",
    "        # direct link to default split\n",
    "        self.split = None\n",
    "\n",
    "    def train_tokenizer(self):\n",
    "        self.toki = tf.keras.preprocessing.text.Tokenizer()\n",
    "        # bz_toki = rec.Itemizer()\n",
    "        # bz_toki.fit_on_texts(stats.itemid.to_list())\n",
    "        self.toki.fit_on_texts(self.items_sorted.itemid.to_list())\n",
    "        _, self.num_words = self.toki.texts_to_matrix(['xx']).shape\n",
    "        print(\"Tokenizer trained for\", self.num_words, \"items.\")\n",
    "\n",
    "    def create_splits(self, n, k_test, shuffle=True, n_fold=True, generators=True, batch_size=1024):\n",
    "        \"\"\"\n",
    "        Create n splits of k test items\n",
    "        shuffle = shuffle users on begin\n",
    "        n_fold = create n disjunct folds of data\n",
    "        \"\"\"\n",
    "        if not len(self.splits) == 0:\n",
    "            print(\"Splits are not empty! Doing nothing...\")\n",
    "            return\n",
    "\n",
    "        print(\"Creating\", n, \"splits of\", k_test, \"samples each.\")\n",
    "        if shuffle:\n",
    "            print(\"Initial user shuffle.\")\n",
    "            self.users = self.users.sample(frac=1, random_state=get_seed())\n",
    "\n",
    "        for i in range(n):\n",
    "            print(\"Creating split nr.\", i + 1)\n",
    "            self.splits.append(\n",
    "                Split(self, k_test, shuffle=False, index=i * k_test, generator=generators, batch_size=batch_size))\n",
    "\n",
    "        self.split = self.splits[0]\n",
    "\n",
    "    def save_splits(self):\n",
    "        for i in range(len(self.splits)):\n",
    "            d = \"split_\" + str(i + 1)\n",
    "            print(d)\n",
    "            os.makedirs(d)\n",
    "            self.splits[i].train_users.to_json(d + \"/train_users.json\")\n",
    "            self.splits[i].test_users.to_json(d + \"/test_users.json\")\n",
    "\n",
    "    def load_splits(self, split=0):\n",
    "        if split == 0:\n",
    "            for i in range(len(self.splits)):\n",
    "                d = \"split_\" + str(i + 1)\n",
    "                print(d)\n",
    "                self.splits[i].train_users = pd.read_json(d + \"/train_users.json\").userid.apply(str).to_frame()\n",
    "                self.splits[i].test_users = pd.read_json(d + \"/test_users.json\").userid.apply(str).to_frame()\n",
    "                self.splits[i].generators()\n",
    "        else:\n",
    "            d = \"split_\" + str(split)\n",
    "            print(d)\n",
    "            self.splits[0].train_users = pd.read_json(d + \"/train_users.json\").userid.apply(str).to_frame()\n",
    "            self.splits[0].test_users = pd.read_json(d + \"/test_users.json\").userid.apply(str).to_frame()\n",
    "            self.splits[0].generators()\n",
    "\n",
    "    def read_users(self, p=''):\n",
    "        print(\"Reading users\" + p)\n",
    "        self.users = pd.read_json(self.directory + 'users' + p + '.json')\n",
    "        self.users['userid'] = self.users.userid.apply(str)\n",
    "\n",
    "    def read_items(self, p=''):\n",
    "        print(\"Reading items\" + p)\n",
    "        self.items = pd.read_json(self.directory + 'items' + p + '.json')\n",
    "        self.items['itemid'] = self.items.itemid.apply(str)\n",
    "\n",
    "    def read_ratings(self, p=''):\n",
    "        print(\"Reading ratings\" + p)\n",
    "        self.ratings = pd.read_json(self.directory + 'ratings' + p + '.json')\n",
    "        self.ratings['userid'] = self.ratings.userid.apply(str)\n",
    "        self.ratings['itemid'] = self.ratings.itemid.apply(str)\n",
    "\n",
    "    def read_purchases(self, p=''):\n",
    "        print(\"Reading purchases\" + p)\n",
    "        self.purchases = pd.read_json(self.directory + 'purchases' + p + '.json')\n",
    "        self.purchases['userid'] = self.purchases.userid.apply(str)\n",
    "        self.purchases['itemid'] = self.purchases.itemid.apply(str)\n",
    "\n",
    "    def read_purchases_txt(self, p=''):\n",
    "        print(\"Reading purchases_txt\" + p)\n",
    "        self.purchases_txt = pd.read_json(self.directory + 'purchases_txt' + p + '.json')\n",
    "        self.purchases_txt['userid'] = self.purchases_txt.userid.apply(str)\n",
    "\n",
    "    def read_items_sorted(self, p=''):\n",
    "        print(\"Reading items_sorted\" + p)\n",
    "        self.items_sorted = pd.read_json(self.directory + 'items_sorted' + p + '.json')\n",
    "        self.items_sorted['itemid'] = self.items_sorted.itemid.apply(str)\n",
    "\n",
    "    def read_users_sorted(self, p=''):\n",
    "        print(\"Reading users_sorted\" + p)\n",
    "        self.users_sorted = pd.read_json(self.directory + 'users_sorted' + p + '.json')\n",
    "        self.users_sorted['userid'] = self.users_sorted.userid.apply(str)\n",
    "\n",
    "    def read_all(self, p=''):\n",
    "        now = time()\n",
    "        self.read_users(p)\n",
    "        self.read_items(p)\n",
    "        # self.read_purchases(p)\n",
    "        self.read_purchases_txt(p)\n",
    "        self.read_items_sorted(p)\n",
    "        self.read_users_sorted(p)\n",
    "        print(\"Read all in\", time() - now)\n",
    "        self.train_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split:\n",
    "    \"\"\"\n",
    "    Definition of train/validation/test subsets of the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, k_test, shuffle=True, index=0, generator=True, batch_size=1024):\n",
    "        \"\"\"\n",
    "        Create split of k test items\n",
    "        shuffle = shuffle users on begin\n",
    "        n_fold = create n disjunct folds of data\n",
    "        \"\"\"\n",
    "        self.master_data = data\n",
    "        if shuffle:\n",
    "            self.all_users = data.users.sample(frac=1).copy(deep=True)\n",
    "        else:\n",
    "            self.all_users = data.users.copy(deep=True)\n",
    "\n",
    "        self.test_users = self.all_users.iloc[index:index + k_test]\n",
    "        self.train_users = self.all_users[~self.all_users.userid.isin(self.test_users.userid)]\n",
    "        self.validation_users = self.train_users.iloc[index:index + k_test]\n",
    "        self.train_users = self.train_users[~self.train_users.userid.isin(self.validation_users.userid)]\n",
    "        if generator:\n",
    "            self.generators(batch_size=batch_size)\n",
    "\n",
    "    def generators(self,\n",
    "                   batch_size=1024,\n",
    "                   random_batching=True,\n",
    "                   prevent_identity=True,\n",
    "                   full_data=False,\n",
    "                   p50_splits=True,\n",
    "                   p2575_splits=False,\n",
    "                   p7525_splits=False,\n",
    "                   p2525_splits=False,\n",
    "                   p7575_splits=False\n",
    "                   ):\n",
    "        self.train_gen = SplitGenerator(\n",
    "            data_df=self.train_purchases_txt(),\n",
    "            itemizer=self.master_data.toki,\n",
    "            batch_size=batch_size,\n",
    "            random_batching=random_batching,\n",
    "            prevent_identity=prevent_identity,\n",
    "            full_data=full_data,\n",
    "            p50_splits=p50_splits,\n",
    "            p2575_splits=p2575_splits,\n",
    "            p7525_splits=p7525_splits,\n",
    "            p2525_splits=p2525_splits,\n",
    "            p7575_splits=p7575_splits\n",
    "        )\n",
    "\n",
    "        self.test_gen = SplitGenerator(\n",
    "            data_df=self.test_purchases_txt(),\n",
    "            itemizer=self.master_data.toki,\n",
    "            batch_size=128,\n",
    "            random_batching=False,\n",
    "            prevent_identity=False,\n",
    "            full_data=True,\n",
    "            p50_splits=False,\n",
    "            p2575_splits=False,\n",
    "            p7525_splits=False,\n",
    "            p2525_splits=False,\n",
    "            p7575_splits=False\n",
    "        )\n",
    "\n",
    "        self.validation_gen = SplitGenerator(\n",
    "            data_df=self.validation_purchases_txt(),\n",
    "            itemizer=self.master_data.toki,\n",
    "            batch_size=128,\n",
    "            random_batching=False,\n",
    "            prevent_identity=False,\n",
    "            full_data=True,\n",
    "            p50_splits=False,\n",
    "            p2575_splits=False,\n",
    "            p7525_splits=False,\n",
    "            p2525_splits=False,\n",
    "            p7575_splits=False\n",
    "        )\n",
    "        print(\"Creating evaluator\")\n",
    "\n",
    "        np.random.seed(get_seed())\n",
    "        random.seed(get_seed())\n",
    "        self.test_evaluator = Evaluator(self, data=\"test\")\n",
    "        np.random.seed(get_seed())\n",
    "        random.seed(get_seed())\n",
    "        self.evaluator = Evaluator(self, data=\"val\")\n",
    "\n",
    "    def train_purchases_txt(self):\n",
    "        return self.master_data.purchases_txt[self.master_data.purchases_txt.userid.isin(self.train_users.userid)].copy(\n",
    "            deep=True)\n",
    "\n",
    "    def test_purchases_txt(self):\n",
    "        return self.master_data.purchases_txt[self.master_data.purchases_txt.userid.isin(self.test_users.userid)].copy(\n",
    "            deep=True)\n",
    "\n",
    "    def validation_purchases_txt(self):\n",
    "        return self.master_data.purchases_txt[\n",
    "            self.master_data.purchases_txt.userid.isin(self.validation_users.userid)].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    TF data generator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_df,\n",
    "            itemizer,\n",
    "            batch_size=128,\n",
    "            random_batching=True,\n",
    "            prevent_identity=False,\n",
    "            full_data=True,\n",
    "            p50_splits=True,\n",
    "            p2575_splits=False,\n",
    "            p7525_splits=False,\n",
    "            p2525_splits=False,\n",
    "            p7575_splits=False):\n",
    "\n",
    "        now = time()\n",
    "        self.prevent_identity = prevent_identity\n",
    "        self.full_data = full_data\n",
    "        self.p50_splits = p50_splits\n",
    "        self.p2575_splits = p2575_splits\n",
    "        self.p7525_splits = p7525_splits\n",
    "        self.p2525_splits = p2525_splits\n",
    "        self.p7575_splits = p7575_splits\n",
    "        self.toki = itemizer\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data_df\n",
    "        # self.data_np = self.data.to_numpy()\n",
    "\n",
    "        self.length = len(self.data)\n",
    "        self.random_batching = random_batching\n",
    "        self.on_epoch_end()\n",
    "        print(\"SplitGenerator init done in\", time() - now, \"secs.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.length / self.batch_size)) - 1\n",
    "\n",
    "    def __call__(self, batch_size):\n",
    "        \"\"\"Allows to use the size of batch when calling the training.\"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        return self\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.random_batching:\n",
    "            self.data = self.data.sample(frac=1)\n",
    "\n",
    "        self.data['temp_itemids_p'] = self.data['itemids'].apply(shufflestr)\n",
    "        self.data['temp_itemids_p1_50'] = self.data['temp_itemids_p'].apply(split1_50)\n",
    "        self.data['temp_itemids_p2_50'] = self.data['temp_itemids_p'].apply(split2_50)\n",
    "        self.data['temp_itemids_p_25'] = self.data['temp_itemids_p'].apply(split25)\n",
    "        self.data['temp_itemids_p_75'] = self.data['temp_itemids_p'].apply(split75)\n",
    "        self.data_np = self.data.to_numpy()\n",
    "\n",
    "    def get_basket_np(self, items):\n",
    "        if self.n_ratings:\n",
    "            return np.vstack([self.embeddings_dict.get(x, self.null_val) for x in items.split(',')])\n",
    "\n",
    "        return np.vstack([self.embeddings_dict.get(x, self.null_val) for x in set(items.split(','))])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # binary mode = output vectors is 0/1 only\n",
    "        mod = 'binary'\n",
    "\n",
    "        data_slice = self.data_np[self.batch_size * index:self.batch_size * index + self.batch_size]\n",
    "\n",
    "        indices = list(range(self.__len__()))\n",
    "        indices += indices\n",
    "\n",
    "        index2 = indices[index + 1]\n",
    "        index3 = indices[index + 2]\n",
    "        index4 = indices[index + 3]\n",
    "        index5 = indices[index + 4]\n",
    "\n",
    "        if self.full_data:\n",
    "            data_slice = self.data_np[self.batch_size * index:self.batch_size * index + self.batch_size]\n",
    "\n",
    "        if self.p50_splits:\n",
    "            data_slice2 = self.data_np[self.batch_size * index2:self.batch_size * index2 + self.batch_size]\n",
    "            data_slice3 = self.data_np[self.batch_size * index3:self.batch_size * index3 + self.batch_size]\n",
    "\n",
    "        if self.p2575_splits or self.p7525_splits or self.p2525_splits or self.p7575_splits:\n",
    "            data_slice4 = self.data_np[self.batch_size * index4:self.batch_size * index4 + self.batch_size]\n",
    "            data_slice5 = self.data_np[self.batch_size * index5:self.batch_size * index5 + self.batch_size]\n",
    "\n",
    "        ret_x = []\n",
    "        ret_y = []\n",
    "\n",
    "        # full input to full_output\n",
    "        if self.full_data:\n",
    "            ret_x.append(self.toki.texts_to_matrix(data_slice[:, 1], mode=mod))\n",
    "            ret_y.append(self.toki.texts_to_matrix(data_slice[:, 1], mode=mod))\n",
    "\n",
    "        if self.p50_splits:\n",
    "            ret_x.append(self.toki.texts_to_matrix(data_slice2[:, 3], mode=mod))\n",
    "            ret_x.append(self.toki.texts_to_matrix(data_slice3[:, 4], mode=mod))\n",
    "\n",
    "            if self.prevent_identity:\n",
    "                ret_y.append(self.toki.texts_to_matrix(data_slice2[:, 4], mode=mod))\n",
    "                ret_y.append(self.toki.texts_to_matrix(data_slice3[:, 3], mode=mod))\n",
    "            else:\n",
    "                ret_y.append(self.toki.texts_to_matrix(data_slice2[:, 3], mode=mod))\n",
    "                ret_y.append(self.toki.texts_to_matrix(data_slice3[:, 4], mode=mod))\n",
    "\n",
    "        if self.p2575_splits:\n",
    "            ret_x.append(self.toki.texts_to_matrix(data_slice4[:, 5], mode=mod))\n",
    "            ret_y.append(self.toki.texts_to_matrix(data_slice4[:, 6], mode=mod))\n",
    "\n",
    "        if self.p7525_splits:\n",
    "            ret_x.append(self.toki.texts_to_matrix(data_slice4[:, 6], mode=mod))\n",
    "            ret_y.append(self.toki.texts_to_matrix(data_slice4[:, 5], mode=mod))\n",
    "\n",
    "        if self.p2525_splits:\n",
    "            ret_x.append(self.toki.texts_to_matrix(data_slice5[:, 5], mode=mod))\n",
    "            ret_y.append(self.toki.texts_to_matrix(data_slice5[:, 5], mode=mod))\n",
    "\n",
    "        if self.p7575_splits:\n",
    "            ret_x.append(self.toki.texts_to_matrix(data_slice5[:, 6], mode=mod))\n",
    "            ret_y.append(self.toki.texts_to_matrix(data_slice5[:, 6], mode=mod))\n",
    "\n",
    "        return np.vstack(ret_x), np.vstack(ret_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Evaluation on give data split.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, method='leave_random_20_pct_out', data=\"test\", debug=False):\n",
    "        assert method in ['leave_random_20_pct_out', '1_20', '2_20', '3_20', '4_20', '5_20']\n",
    "        self.split = split\n",
    "        if data == \"val\":\n",
    "            print(\"Creating validation split evaluator with\", method, \"method.\")\n",
    "            self.ivx = split.validation_gen.data.set_index(split.validation_gen.data.userid).sort_index().itemids.apply(\n",
    "                lambda x: x.split(','))\n",
    "        else:\n",
    "            print(\"Creating test split evaluator with\", method, \"method.\")\n",
    "            self.ivx = split.test_gen.data.set_index(split.test_gen.data.userid).sort_index().itemids.apply(\n",
    "                lambda x: x.split(','))\n",
    "        if debug:\n",
    "            print(\"Stage 1 done.\")\n",
    "        self.tpx = []\n",
    "        if method == 'leave_random_20_pct_out':\n",
    "            for e in range(len(self.ivx)):\n",
    "                tech20 = []\n",
    "                num_to_add = int(len(self.ivx[e]) * 0.2)\n",
    "                if num_to_add < 1:\n",
    "                    num_to_add = 1\n",
    "                for x in range(num_to_add):\n",
    "                    random_pick_index = randrange(len(self.ivx[e]))\n",
    "                    tech20.append(self.ivx[e].pop(random_pick_index))\n",
    "                self.tpx.append(tech20)\n",
    "        else:\n",
    "            end = int(method[0])\n",
    "            start = end - 1\n",
    "            random.seed(get_seed())\n",
    "            for e in range(len(self.ivx)):\n",
    "                tech20 = []\n",
    "                interactions_len = len(self.ivx[e])\n",
    "                num_to_add = int(interactions_len * 0.2)\n",
    "                if num_to_add < 1:\n",
    "                    num_to_add = 1\n",
    "                shuffle(self.ivx[e])\n",
    "                for x in range(start * num_to_add, end * num_to_add):\n",
    "                    random_pick_index = x\n",
    "                    if random_pick_index >= len(self.ivx[e]):\n",
    "                        random_pick_index = len(self.ivx[e]) - 1\n",
    "                    tech20.append(self.ivx[e].pop(random_pick_index))\n",
    "                self.tpx.append(tech20)\n",
    "\n",
    "        if debug:\n",
    "            print(\"Stage 2 done.\")\n",
    "        self.iv = self.split.master_data.toki.texts_to_matrix(\n",
    "            [\",\".join(x) for x in self.ivx]\n",
    "        )\n",
    "        if debug:\n",
    "            print(\"Stage 3 done.\")\n",
    "        self.tp = self.split.master_data.toki.texts_to_matrix(\n",
    "            [\",\".join(x) for x in self.tpx]\n",
    "        )\n",
    "        self.tpx_set = [set(t) for t in self.tpx]\n",
    "        if debug:\n",
    "            print(\"Stage 4 done.\")\n",
    "\n",
    "    def update(self, m, chunk=1000):\n",
    "        assert len(self.iv) % chunk == 0\n",
    "        self.pr = np.vstack([m.predict(self.iv[chunk * x:chunk * (x + 1)]) for x in range(len(self.iv) // chunk)])\n",
    "        self.ppp = (1 - self.iv) * self.pr\n",
    "        self.ppp[:, 0] = 0\n",
    "\n",
    "    def get_ncdg(self, k):\n",
    "        pr = self.pr\n",
    "        ppp = self.ppp\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        topk_part = ppp[np.arange(ppp.shape[0])[:, np.newaxis], idx[:, :k]]\n",
    "        idx_part = np.argsort(-topk_part, axis=1)\n",
    "        idx_topk = idx[np.arange(self.iv.shape[0])[:, np.newaxis], idx_part]\n",
    "        tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "        z = zip([[self.split.master_data.toki.index_word[b] for b in a] for a in idx_topk], self.tpx_set)\n",
    "        n = np.array([(np.array([1 if x in true else 0 for x in pred]) * tp).sum() for pred, true in z])\n",
    "        d = np.array([(np.ones(min(k, len(x))) * tp[:len(x)]).sum() for x in self.tpx_set])\n",
    "        return (n / d).mean()\n",
    "\n",
    "    def get_hr(self, k):\n",
    "        pr = self.pr\n",
    "        ppp = self.ppp\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        z = zip([set([self.split.master_data.toki.index_word[s] for s in a[:k]]) for a in idx], self.tpx_set)\n",
    "        r = [0 if len(pred & true) / min(k, len(true)) == 0 else 1 for pred, true in z]\n",
    "        return sum(r) / len(r)\n",
    "\n",
    "    def get_coverage(self, k):\n",
    "        pr = self.pr\n",
    "        ppp = self.ppp\n",
    "\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        covered = len(set().union(*[set([self.split.master_data.toki.index_word[s] for s in a[:k]]) for a in idx]))\n",
    "        total = self.iv.shape[1] - 1\n",
    "        return covered / total\n",
    "\n",
    "    def get_recall(self, k):\n",
    "        pr = self.pr\n",
    "        ppp = self.ppp\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        z = zip([set([self.split.master_data.toki.index_word[s] for s in a[:k]]) for a in idx], self.tpx_set)\n",
    "        r = [len(pred & true) / min(k, len(true)) for pred, true in z]\n",
    "        return sum(r) / len(r)\n",
    "\n",
    "    def ncdg(self, m, k):\n",
    "        c = getattr(m, \"pred_from_mean\", None)\n",
    "        if not callable(c):\n",
    "            c = getattr(m, \"predict\", None)\n",
    "        pr = c(self.iv)\n",
    "        ppp = (1 - self.iv) * pr\n",
    "        ppp[:, 0] = 0\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        topk_part = ppp[np.arange(ppp.shape[0])[:, np.newaxis], idx[:, :k]]\n",
    "        idx_part = np.argsort(-topk_part, axis=1)\n",
    "        idx_topk = idx[np.arange(self.iv.shape[0])[:, np.newaxis], idx_part]\n",
    "        tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "        z = zip([[self.split.master_data.toki.index_word[b] for b in a] for a in idx_topk], self.tpx_set)\n",
    "        n = np.array([(np.array([1 if x in true else 0 for x in pred]) * tp).sum() for pred, true in z])\n",
    "        d = np.array([(np.ones(min(k, len(x))) * tp[:len(x)]).sum() for x in self.tpx_set])\n",
    "        return (n / d).mean()\n",
    "\n",
    "    def hr(self, m, k):\n",
    "        c = getattr(m, \"pred_from_mean\", None)\n",
    "        if not callable(c):\n",
    "            c = getattr(m, \"predict\", None)\n",
    "        pr = c(self.iv)\n",
    "        ppp = (1 - self.iv) * pr\n",
    "        ppp[:, 0] = 0\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        z = zip([set([self.split.master_data.toki.index_word[s] for s in a[:k]]) for a in idx], self.tpx_set)\n",
    "        r = [0 if len(pred & true) / min(k, len(true)) == 0 else 1 for pred, true in z]\n",
    "        return sum(r) / len(r)\n",
    "\n",
    "    def coverage(self, m, k):\n",
    "        c = getattr(m, \"pred_from_mean\", None)\n",
    "        if not callable(c):\n",
    "            c = getattr(m, \"predict\", None)\n",
    "        pr = c(self.iv)\n",
    "        ppp = (1 - self.iv) * pr\n",
    "        ppp[:, 0] = 0\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        covered = len(set().union(*[set([self.split.master_data.toki.index_word[s] for s in a[:k]]) for a in idx]))\n",
    "        total = self.iv.shape[1] - 1\n",
    "        return covered / total\n",
    "\n",
    "    def recall(self, m, k):\n",
    "        c = getattr(m, \"pred_from_mean\", None)\n",
    "        if not callable(c):\n",
    "            c = getattr(m, \"predict\", None)\n",
    "        pr = c(self.iv)\n",
    "        ppp = (1 - self.iv) * pr\n",
    "        ppp[:, 0] = 0\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        z = zip([set([self.split.master_data.toki.index_word[s] for s in a[:k]]) for a in idx], self.tpx_set)\n",
    "        r = [len(pred & true) / min(k, len(true)) for pred, true in z]\n",
    "        return sum(r) / len(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model abstract class\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    Abstract model.\n",
    "    Subclassed model should implements create_model and train_model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, name):\n",
    "        self.split = split\n",
    "        self.dataset = split.master_data\n",
    "        self.metrics = {\n",
    "            'Recall@5': {'k': 5, 'method': self.split.evaluator.get_recall, 'value': None},\n",
    "            'Recall@20': {'k': 20, 'method': self.split.evaluator.get_recall, 'value': None},\n",
    "            'Recall@50': {'k': 50, 'method': self.split.evaluator.get_recall, 'value': None},\n",
    "            'NCDG@100': {'k': 100, 'method': self.split.evaluator.get_ncdg, 'value': None},\n",
    "            'Coverage@5': {'k': 5, 'method': self.split.evaluator.get_coverage, 'value': None},\n",
    "            'Coverage@20': {'k': 20, 'method': self.split.evaluator.get_coverage, 'value': None},\n",
    "            'Coverage@50': {'k': 50, 'method': self.split.evaluator.get_coverage, 'value': None},\n",
    "            'Coverage@100': {'k': 100, 'method': self.split.evaluator.get_coverage, 'value': None},\n",
    "        }\n",
    "        self.name = name\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Build Your own model here\n",
    "        \"\"\"\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Create your own training loop here\n",
    "        \"\"\"\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        self.split.evaluator.update(self.model)\n",
    "        for x in self.metrics.values():\n",
    "            x['value'] = x['method'](x['k'])\n",
    "\n",
    "    def print_metrics(self):\n",
    "        print(\"Model metrics:\", end='')\n",
    "        for k, x in self.metrics.items():\n",
    "            print(k, end=\"=\")\n",
    "            print(round(x['value'], 4), end=\" \")\n",
    "        print()\n",
    "\n",
    "    def test_model(self):\n",
    "        e = self.split.test_evaluator\n",
    "        e.update(self.model)\n",
    "        print(\"Results for test set: Recall@20=\", e.get_recall(20), \", Recall@50=\", e.get_recall(50), \", NCDG@100=\",\n",
    "              e.get_ncdg(100), sep=\"\")\n",
    "        with open(\"seed_results_test.txt\", \"a\") as myfile:\n",
    "            myfile.write(\"Results for test set: Recall@20=\" + str(e.get_recall(20)) + \", Recall@50=\" + str(\n",
    "                e.get_recall(50)) + \", NCDG@100=\" + str(e.get_ncdg(100)) + \"\\n\")\n",
    "\n",
    "    def test_model_val(self):\n",
    "        e = self.split.evaluator\n",
    "        e.update(self.model)\n",
    "        print(\"Results for validation set: Recall@20=\", e.get_recall(20), \", Recall@50=\", e.get_recall(50),\n",
    "              \", NCDG@100=\",\n",
    "              e.get_ncdg(100), sep=\"\")\n",
    "        with open(\"seed_results_val.txt\", \"a\") as myfile:\n",
    "            myfile.write(\"Results for validation set: Recall@20=\" + str(e.get_recall(20)) + \", Recall@50=\" + str(\n",
    "                e.get_recall(50)) + \", NCDG@100=\" + str(e.get_ncdg(100)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow objects - Callbacks\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Evaluate model in tf callback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rsmodel):\n",
    "        super(MetricsCallback, self).__init__()\n",
    "        self.epoch = 0\n",
    "        self.loss_metrics = dict()\n",
    "        self.eval_metrics = dict()\n",
    "        self.evaluate_loss_metrics = ['loss', 'val_loss']\n",
    "        self.rsmodel = rsmodel\n",
    "        self.best_ncdg100 = 0.\n",
    "        self.best_ncdg100_epoch = 0\n",
    "        self.best_recall20 = 0.\n",
    "        self.best_recall20_epoch = 0\n",
    "        self.best_recall50 = 0.\n",
    "        self.best_recall50_epoch = 0\n",
    "        self.tsne_df = pd.DataFrame(columns=[\"epoch\", \"tsne_coords\"])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch += 1\n",
    "\n",
    "        self.loss_metrics[self.epoch] = dict()\n",
    "        self.eval_metrics[self.epoch] = dict()\n",
    "        # add metrics from logs\n",
    "        for x in self.evaluate_loss_metrics:\n",
    "            self.loss_metrics[self.epoch][x] = logs[x]\n",
    "        # add custom metrics\n",
    "        self.rsmodel.evaluate_model()\n",
    "        self.rsmodel.print_metrics()\n",
    "        for x in self.rsmodel.metrics.keys():\n",
    "            self.eval_metrics[self.epoch][x] = self.rsmodel.metrics[x]['value']\n",
    "        self.ncdg_100_watch()\n",
    "        self.recall20_watch()\n",
    "        self.recall50_watch()\n",
    "        # self.get_history_df()\n",
    "        # self.calc_tsne()\n",
    "\n",
    "    def recall20_watch(self):\n",
    "        if self.eval_metrics[self.epoch]['Recall@20'] > self.best_recall20:\n",
    "            print(\"New best for Recall@20\")\n",
    "            self.model.save_weights(self.rsmodel.name + \"_best_recall_20/\" + self.rsmodel.name)\n",
    "            self.best_recall20 = self.eval_metrics[self.epoch]['Recall@20']\n",
    "            self.best_recall20_epoch = self.epoch\n",
    "            with open(self.rsmodel.name + \"_best_recall_20/\" + \"epoch.txt\", \"w\") as text_file:\n",
    "                text_file.write(str(self.best_recall20_epoch))\n",
    "\n",
    "    def recall50_watch(self):\n",
    "        if self.eval_metrics[self.epoch]['Recall@50'] > self.best_recall50:\n",
    "            print(\"New best for Recall@50\")\n",
    "            self.model.save_weights(self.rsmodel.name + \"_best_recall_50/\" + self.rsmodel.name)\n",
    "            self.best_recall50 = self.eval_metrics[self.epoch]['Recall@50']\n",
    "            self.best_recall50_epoch = self.epoch\n",
    "            with open(self.rsmodel.name + \"_best_recall_50/\" + \"epoch.txt\", \"w\") as text_file:\n",
    "                text_file.write(str(self.best_recall50_epoch))\n",
    "\n",
    "    def ncdg_100_watch(self):\n",
    "        if self.eval_metrics[self.epoch]['NCDG@100'] > self.best_ncdg100:\n",
    "            print(\"New best for NCDG@100\")\n",
    "            self.model.save_weights(self.rsmodel.name + \"_best_ncdg_100/\" + self.rsmodel.name)\n",
    "            self.best_ncdg100 = self.eval_metrics[self.epoch]['NCDG@100']\n",
    "            self.best_ncdg100_epoch = self.epoch\n",
    "            with open(self.rsmodel.name + \"_best_ncdg_100/\" + \"epoch.txt\", \"w\") as text_file:\n",
    "                text_file.write(str(self.best_ncdg100_epoch))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.plot_history()\n",
    "\n",
    "    def get_history_df(self):\n",
    "\n",
    "        outt1 = {\n",
    "            'epochs': [x for x in self.rsmodel.mc.loss_metrics.keys()]\n",
    "        }\n",
    "\n",
    "        outt2 = {\n",
    "            'epochs': [x for x in self.rsmodel.mc.eval_metrics.keys()]\n",
    "        }\n",
    "\n",
    "        for k in self.loss_metrics[1].keys():\n",
    "            outt1[k] = [self.loss_metrics[x][k] for x in self.loss_metrics.keys()]\n",
    "\n",
    "        for k in self.eval_metrics[1].keys():\n",
    "            outt2[k] = [self.eval_metrics[x][k] for x in self.eval_metrics.keys()]\n",
    "\n",
    "        self.history_loss_df = pd.DataFrame(outt1)\n",
    "        self.history_loss_df.to_json(self.rsmodel.name + \"_loss.json\")\n",
    "\n",
    "        self.history_df = pd.DataFrame(outt2)\n",
    "        self.history_df.to_json(self.rsmodel.name + \"_metrics.json\")\n",
    "\n",
    "        return self.history_df\n",
    "\n",
    "    def plot_history(self):\n",
    "        return self.get_history_df().set_index(self.history_df.epochs, drop=True).iloc[:, 1:].plot(figsize=(20, 10))\n",
    "\n",
    "    def calc_tsne(self):\n",
    "        num_words = self.rsmodel.dataset.num_words\n",
    "        input_single_item_matrix = np.zeros((num_words, num_words))\n",
    "        np.fill_diagonal(input_single_item_matrix, 1.)\n",
    "        qqq = scale_d(self.model.predict(input_single_item_matrix)).numpy() * .99\n",
    "        np.fill_diagonal(qqq, 1.)\n",
    "        tsne_coordinates = TSNE(n_components=2, metric=\"precomputed\", angle=0.5, perplexity=30, random_state=6).fit(\n",
    "            (1 - qqq))\n",
    "        tsne_coordinates = tsne_coordinates.embedding_\n",
    "        self.tsne_df.loc[self.epoch] = [self.epoch, tsne_coordinates]\n",
    "        self.tsne_df.to_json(self.rsmodel.name + \"_tsne.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalToZero(tf.keras.constraints.Constraint):\n",
    "    def __call__(self, w):\n",
    "        \"\"\"Set diagonal to zero\"\"\"\n",
    "        q = tf.linalg.set_diag(w, tf.zeros(w.shape[0:-1]), name=None)\n",
    "        return q\n",
    "\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a basket.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim), stddev=1.)\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "class VASP(Model):\n",
    "    class Model(tf.keras.Model):\n",
    "        def __init__(self, num_words, latent=1024, hidden=1024, items_sampling=1.):\n",
    "            \"\"\"\n",
    "            num_words             nr of items in dataset (size of tokenizer)\n",
    "            latent                size of latent space\n",
    "            hidden                size of hidden layers\n",
    "            items_sampling        Large items datatsets can be very gpu memory consuming in EASE layer.\n",
    "                                  This coefficient reduces number of ease parametrs by taking only\n",
    "                                  fraction of items sorted by popularity as input for model.\n",
    "                                  Note: This coef should be somewhere around coverage@100 achieved by full\n",
    "                                  size model.\n",
    "                                  For ML20M this coef should be between 0.4888 (coverage@100 for full model)\n",
    "                                  and 1.0\n",
    "                                  For Netflix this coef should be between 0.7055 (coverage@100 for full\n",
    "                                  model) and 1.0\n",
    "            \"\"\"\n",
    "            super(VASP.Model, self).__init__()\n",
    "\n",
    "            self.sampled_items = int(num_words * items_sampling)\n",
    "\n",
    "            assert self.sampled_items > 0\n",
    "            assert self.sampled_items <= num_words\n",
    "\n",
    "            self.s = self.sampled_items < num_words\n",
    "\n",
    "            # ************* ENCODER ***********************\n",
    "            self.encoder1 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln1 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder2 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln2 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder3 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln3 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder4 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln4 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder5 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln5 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder6 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln6 = tf.keras.layers.LayerNormalization()\n",
    "            self.encoder7 = tf.keras.layers.Dense(hidden)\n",
    "            self.ln7 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "            # ************* SAMPLING **********************\n",
    "            self.dense_mean = tf.keras.layers.Dense(latent,\n",
    "                                                    name=\"Mean\")\n",
    "            self.dense_log_var = tf.keras.layers.Dense(latent,\n",
    "                                                       name=\"log_var\")\n",
    "\n",
    "            self.sampling = Sampling(name='Sampler')\n",
    "\n",
    "            # ************* DECODER ***********************\n",
    "            self.decoder1 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln1 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder2 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln2 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder3 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln3 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder4 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln4 = tf.keras.layers.LayerNormalization()\n",
    "            self.decoder5 = tf.keras.layers.Dense(hidden)\n",
    "            self.dln5 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "            self.decoder_resnet = tf.keras.layers.Dense(self.sampled_items,\n",
    "                                                        activation='sigmoid',\n",
    "                                                        name=\"DecoderR\")\n",
    "            self.decoder_latent = tf.keras.layers.Dense(self.sampled_items,\n",
    "                                                        activation='sigmoid',\n",
    "                                                        name=\"DecoderL\")\n",
    "\n",
    "            # ************* PARALLEL SHALLOW PATH *********\n",
    "\n",
    "            self.ease = tf.keras.layers.Dense(\n",
    "                self.sampled_items,\n",
    "                activation='sigmoid',\n",
    "                use_bias=False,\n",
    "                kernel_constraint=DiagonalToZero(),  # critical to prevent learning simple identity\n",
    "            )\n",
    "\n",
    "        def call(self, x, training=None):\n",
    "            sampling = self.s\n",
    "            if sampling:\n",
    "                sampled_x = x[:, :self.sampled_items]\n",
    "                non_sampled = x[:, self.sampled_items:] * 0.\n",
    "            else:\n",
    "                sampled_x = x\n",
    "\n",
    "            z_mean, z_log_var, z = self.encode(sampled_x)\n",
    "            if training:\n",
    "                d = self.decode(z)\n",
    "                # Add KL divergence regularization loss.\n",
    "                kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "                kl_loss = tf.reduce_mean(kl_loss)\n",
    "                kl_loss *= -0.5\n",
    "                self.add_loss(kl_loss)\n",
    "                self.add_metric(kl_loss, name=\"kl_div\")\n",
    "            else:\n",
    "                d = self.decode(z_mean)\n",
    "\n",
    "            if sampling:\n",
    "                d = tf.concat([d, non_sampled], axis=-1)\n",
    "\n",
    "            ease = self.ease(sampled_x)\n",
    "\n",
    "            if sampling:\n",
    "                ease = tf.concat([ease, non_sampled], axis=-1)\n",
    "\n",
    "            return d * ease\n",
    "\n",
    "        def decode(self, x):\n",
    "            e0 = x\n",
    "            e1 = self.dln1(tf.keras.activations.swish(self.decoder1(e0)))\n",
    "            e2 = self.dln2(tf.keras.activations.swish(self.decoder2(e1) + e1))\n",
    "            e3 = self.dln3(tf.keras.activations.swish(self.decoder3(e2) + e1 + e2))\n",
    "            e4 = self.dln4(tf.keras.activations.swish(self.decoder4(e3) + e1 + e2 + e3))\n",
    "            e5 = self.dln5(tf.keras.activations.swish(self.decoder5(e4) + e1 + e2 + e3 + e4))\n",
    "\n",
    "            dr = self.decoder_resnet(e5)\n",
    "            dl = self.decoder_latent(x)\n",
    "\n",
    "            return dr * dl\n",
    "\n",
    "        def encode(self, x):\n",
    "            e0 = x\n",
    "            e1 = self.ln1(tf.keras.activations.swish(self.encoder1(e0)))\n",
    "            e2 = self.ln2(tf.keras.activations.swish(self.encoder2(e1) + e1))\n",
    "            e3 = self.ln3(tf.keras.activations.swish(self.encoder3(e2) + e1 + e2))\n",
    "            e4 = self.ln4(tf.keras.activations.swish(self.encoder4(e3) + e1 + e2 + e3))\n",
    "            e5 = self.ln5(tf.keras.activations.swish(self.encoder5(e4) + e1 + e2 + e3 + e4))\n",
    "            e6 = self.ln6(tf.keras.activations.swish(self.encoder6(e5) + e1 + e2 + e3 + e4 + e5))\n",
    "            e7 = self.ln7(tf.keras.activations.swish(self.encoder7(e6) + e1 + e2 + e3 + e4 + e5 + e6))\n",
    "\n",
    "            z_mean = self.dense_mean(e7)\n",
    "            z_log_var = self.dense_log_var(e7)\n",
    "            z = self.sampling((z_mean, z_log_var))\n",
    "\n",
    "            return z_mean, z_log_var, z\n",
    "\n",
    "    def create_model(self, latent=2048, hidden=4096, ease_items_sampling=1., summary=False):\n",
    "        self.model = VASP.Model(self.dataset.num_words, latent, hidden, ease_items_sampling)\n",
    "        self.model(self.split.train_gen[0][0])\n",
    "        if summary:\n",
    "            self.model.summary()\n",
    "        self.mc = MetricsCallback(self)\n",
    "\n",
    "    def compile_model(self, lr=0.00002, fl_alpha=0.25, fl_gamma=2.0):\n",
    "        \"\"\"\n",
    "        lr         learning rate of Nadam optimizer\n",
    "        fl_alpha   alpha parameter of focal crossentropy\n",
    "        fl_gamma   gamma parameter of focal crossentropy\n",
    "        \"\"\"\n",
    "        self.model.compile(\n",
    "            optimizer=tf.keras.optimizers.Nadam(lr),\n",
    "            loss=lambda x, y: tfa.losses.sigmoid_focal_crossentropy(x, y, alpha=fl_alpha, gamma=fl_gamma),\n",
    "            metrics=['mse', cosine_loss]\n",
    "        )\n",
    "\n",
    "    def train_model(self, epochs=150):\n",
    "        self.model.fit(\n",
    "            self.split.train_gen,\n",
    "            validation_data=self.split.validation_gen,\n",
    "            epochs=epochs,\n",
    "            callbacks=[self.mc]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset, splits, generators, evaluator and abstract model handlers\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "import bottleneck as bn\n",
    "from random import randrange, shuffle\n",
    "import random\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "SEED = DEFAULT_SEED\n",
    "# Report only TF errors by default\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "\n",
    "\n",
    "# Auxiliary functions\n",
    "\n",
    "def set_seed(seed=DEFAULT_SEED):\n",
    "    \"\"\"\n",
    "    Set random seed in all used libs.\n",
    "    \"\"\"\n",
    "    global SEED\n",
    "    SEED = seed\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "def get_seed():\n",
    "    return SEED\n",
    "\n",
    "\n",
    "def shufflestr(x):\n",
    "    \"\"\"\n",
    "    Shuffle randomly items in string separated by comma.\n",
    "    \"\"\"\n",
    "    p = x.split(',')\n",
    "    random.shuffle(p)\n",
    "    return \",\".join(p)\n",
    "\n",
    "\n",
    "def split2_50(x):\n",
    "    \"\"\"\n",
    "    Returns first half of items in string separated by comma.\n",
    "    \"\"\"\n",
    "    p = x.split(',')\n",
    "    s = int(len(p) * .5)\n",
    "    return \",\".join(p[:s])\n",
    "\n",
    "\n",
    "def split1_50(x):\n",
    "    \"\"\"\n",
    "    Returns second half of items in string separated by comma.\n",
    "    \"\"\"\n",
    "    p = x.split(',')\n",
    "    s = int(len(p) * .5)\n",
    "    return \",\".join(p[s:])\n",
    "\n",
    "\n",
    "def split75(x):\n",
    "    \"\"\"\n",
    "    Returns first three quarters of items in string separated by comma.\n",
    "    \"\"\"\n",
    "    p = x.split(',')\n",
    "    s = int(len(p) * .75)\n",
    "    return \",\".join(p[:s])\n",
    "\n",
    "\n",
    "def split25(x):\n",
    "    \"\"\"\n",
    "    Returns last quarter of items in string separated by comma.\n",
    "    \"\"\"\n",
    "    p = x.split(',')\n",
    "    s = int(len(p) * .75)\n",
    "    return \",\".join(p[s:])\n",
    "\n",
    "\n",
    "# TF functions\n",
    "\n",
    "@tf.function\n",
    "def cosmse(x, y):\n",
    "    # x=tf.cast(x,'float32')\n",
    "    # y=tf.cast(y,'float32')\n",
    "    a = tf.constant(ALPHA) * cosine_loss(x, y)\n",
    "    d = tf.constant(BETA) * tf.keras.losses.MSE(x, y)\n",
    "    return a + d\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def cosine_loss(x, y):\n",
    "    return tf.keras.losses.cosine_similarity(x, y) + tf.constant(1.0)\n",
    "\n",
    "\n",
    "# Dataset classes\n",
    "class Data:\n",
    "    \"\"\"\n",
    "    Basic dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d=str(), pruning=False):\n",
    "        self.directory = d\n",
    "        if pruning:\n",
    "            self.read_all(p=\"_p\" + pruning)\n",
    "        else:\n",
    "            self.read_all()\n",
    "        # array of all data splits\n",
    "        self.splits = []\n",
    "        # direct link to default split\n",
    "        self.split = None\n",
    "\n",
    "    def train_tokenizer(self):\n",
    "        self.toki = tf.keras.preprocessing.text.Tokenizer()\n",
    "        # bz_toki = rec.Itemizer()\n",
    "        # bz_toki.fit_on_texts(stats.itemid.to_list())\n",
    "        self.toki.fit_on_texts(self.items_sorted.itemid.to_list())\n",
    "        _, self.num_words = self.toki.texts_to_matrix(['xx']).shape\n",
    "        print(\"Tokenizer trained for\", self.num_words, \"items.\")\n",
    "\n",
    "    def create_splits(self, n, k_test, shuffle=True, n_fold=True, generators=True, batch_size=1024):\n",
    "        \"\"\"\n",
    "        Create n splits of k test items\n",
    "        shuffle = shuffle users on begin\n",
    "        n_fold = create n disjunct folds of data\n",
    "        \"\"\"\n",
    "        if not len(self.splits) == 0:\n",
    "            print(\"Splits are not empty! Doing nothing...\")\n",
    "            return\n",
    "\n",
    "        print(\"Creating\", n, \"splits of\", k_test, \"samples each.\")\n",
    "        if shuffle:\n",
    "            print(\"Initial user shuffle.\")\n",
    "            self.users = self.users.sample(frac=1, random_state=get_seed())\n",
    "\n",
    "        for i in range(n):\n",
    "            print(\"Creating split nr.\", i + 1)\n",
    "            self.splits.append(\n",
    "                Split(self, k_test, shuffle=False, index=i * k_test, generator=generators, batch_size=batch_size))\n",
    "\n",
    "        self.split = self.splits[0]\n",
    "\n",
    "    def save_splits(self):\n",
    "        for i in range(len(self.splits)):\n",
    "            d = \"split_\" + str(i + 1)\n",
    "            print(d)\n",
    "            os.makedirs(d)\n",
    "            self.splits[i].train_users.to_json(d + \"/train_users.json\")\n",
    "            self.splits[i].test_users.to_json(d + \"/test_users.json\")\n",
    "\n",
    "    def load_splits(self, split=0):\n",
    "        if split == 0:\n",
    "            for i in range(len(self.splits)):\n",
    "                d = \"split_\" + str(i + 1)\n",
    "                print(d)\n",
    "                self.splits[i].train_users = pd.read_json(d + \"/train_users.json\").userid.apply(str).to_frame()\n",
    "                self.splits[i].test_users = pd.read_json(d + \"/test_users.json\").userid.apply(str).to_frame()\n",
    "                self.splits[i].generators()\n",
    "        else:\n",
    "            d = \"split_\" + str(split)\n",
    "            print(d)\n",
    "            self.splits[0].train_users = pd.read_json(d + \"/train_users.json\").userid.apply(str).to_frame()\n",
    "            self.splits[0].test_users = pd.read_json(d + \"/test_users.json\").userid.apply(str).to_frame()\n",
    "            self.splits[0].generators()\n",
    "\n",
    "    def read_users(self, p=''):\n",
    "        print(\"Reading users\" + p)\n",
    "        self.users = pd.read_json(self.directory + 'users' + p + '.json')\n",
    "        self.users['userid'] = self.users.userid.apply(str)\n",
    "\n",
    "    def read_items(self, p=''):\n",
    "        print(\"Reading items\" + p)\n",
    "        self.items = pd.read_json(self.directory + 'items' + p + '.json')\n",
    "        self.items['itemid'] = self.items.itemid.apply(str)\n",
    "\n",
    "    def read_ratings(self, p=''):\n",
    "        print(\"Reading ratings\" + p)\n",
    "        self.ratings = pd.read_json(self.directory + 'ratings' + p + '.json')\n",
    "        self.ratings['userid'] = self.ratings.userid.apply(str)\n",
    "        self.ratings['itemid'] = self.ratings.itemid.apply(str)\n",
    "\n",
    "    def read_purchases(self, p=''):\n",
    "        print(\"Reading purchases\" + p)\n",
    "        self.purchases = pd.read_json(self.directory + 'purchases' + p + '.json')\n",
    "        self.purchases['userid'] = self.purchases.userid.apply(str)\n",
    "        self.purchases['itemid'] = self.purchases.itemid.apply(str)\n",
    "\n",
    "    def read_purchases_txt(self, p=''):\n",
    "        print(\"Reading purchases_txt\" + p)\n",
    "        self.purchases_txt = pd.read_json(self.directory + 'purchases_txt' + p + '.json')\n",
    "        self.purchases_txt['userid'] = self.purchases_txt.userid.apply(str)\n",
    "\n",
    "    def read_items_sorted(self, p=''):\n",
    "        print(\"Reading items_sorted\" + p)\n",
    "        self.items_sorted = pd.read_json(self.directory + 'items_sorted' + p + '.json')\n",
    "        self.items_sorted['itemid'] = self.items_sorted.itemid.apply(str)\n",
    "\n",
    "    def read_users_sorted(self, p=''):\n",
    "        print(\"Reading users_sorted\" + p)\n",
    "        self.users_sorted = pd.read_json(self.directory + 'users_sorted' + p + '.json')\n",
    "        self.users_sorted['userid'] = self.users_sorted.userid.apply(str)\n",
    "\n",
    "    def read_all(self, p=''):\n",
    "        now = time()\n",
    "        self.read_users(p)\n",
    "        self.read_items(p)\n",
    "        # self.read_purchases(p)\n",
    "        self.read_purchases_txt(p)\n",
    "        self.read_items_sorted(p)\n",
    "        self.read_users_sorted(p)\n",
    "        print(\"Read all in\", time() - now)\n",
    "        self.train_tokenizer()\n",
    "\n",
    "\n",
    "class Split:\n",
    "    \"\"\"\n",
    "    Definition of train/validation/test subsets of the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, k_test, shuffle=True, index=0, generator=True, batch_size=1024):\n",
    "        \"\"\"\n",
    "        Create split of k test items\n",
    "        shuffle = shuffle users on begin\n",
    "        n_fold = create n disjunct folds of data\n",
    "        \"\"\"\n",
    "        self.master_data = data\n",
    "        if shuffle:\n",
    "            self.all_users = data.users.sample(frac=1).copy(deep=True)\n",
    "        else:\n",
    "            self.all_users = data.users.copy(deep=True)\n",
    "\n",
    "        self.test_users = self.all_users.iloc[index:index + k_test]\n",
    "        self.train_users = self.all_users[~self.all_users.userid.isin(self.test_users.userid)]\n",
    "        self.validation_users = self.train_users.iloc[index:index + k_test]\n",
    "        self.train_users = self.train_users[~self.train_users.userid.isin(self.validation_users.userid)]\n",
    "        if generator:\n",
    "            self.generators(batch_size=batch_size)\n",
    "\n",
    "    def generators(self,\n",
    "                   batch_size=1024,\n",
    "                   random_batching=True,\n",
    "                   prevent_identity=True,\n",
    "                   full_data=False,\n",
    "                   p50_splits=True,\n",
    "                   p2575_splits=False,\n",
    "                   p7525_splits=False,\n",
    "                   p2525_splits=False,\n",
    "                   p7575_splits=False\n",
    "                   ):\n",
    "        self.train_gen = SplitGenerator(\n",
    "            data_df=self.train_purchases_txt(),\n",
    "            itemizer=self.master_data.toki,\n",
    "            batch_size=batch_size,\n",
    "            random_batching=random_batching,\n",
    "            prevent_identity=prevent_identity,\n",
    "            full_data=full_data,\n",
    "            p50_splits=p50_splits,\n",
    "            p2575_splits=p2575_splits,\n",
    "            p7525_splits=p7525_splits,\n",
    "            p2525_splits=p2525_splits,\n",
    "            p7575_splits=p7575_splits\n",
    "        )\n",
    "\n",
    "        self.test_gen = SplitGenerator(\n",
    "            data_df=self.test_purchases_txt(),\n",
    "            itemizer=self.master_data.toki,\n",
    "            batch_size=128,\n",
    "            random_batching=False,\n",
    "            prevent_identity=False,\n",
    "            full_data=True,\n",
    "            p50_splits=False,\n",
    "            p2575_splits=False,\n",
    "            p7525_splits=False,\n",
    "            p2525_splits=False,\n",
    "            p7575_splits=False\n",
    "        )\n",
    "\n",
    "        self.validation_gen = SplitGenerator(\n",
    "            data_df=self.validation_purchases_txt(),\n",
    "            itemizer=self.master_data.toki,\n",
    "            batch_size=128,\n",
    "            random_batching=False,\n",
    "            prevent_identity=False,\n",
    "            full_data=True,\n",
    "            p50_splits=False,\n",
    "            p2575_splits=False,\n",
    "            p7525_splits=False,\n",
    "            p2525_splits=False,\n",
    "            p7575_splits=False\n",
    "        )\n",
    "        print(\"Creating evaluator\")\n",
    "\n",
    "        np.random.seed(get_seed())\n",
    "        random.seed(get_seed())\n",
    "        self.test_evaluator = Evaluator(self, data=\"test\")\n",
    "        np.random.seed(get_seed())\n",
    "        random.seed(get_seed())\n",
    "        self.evaluator = Evaluator(self, data=\"val\")\n",
    "\n",
    "    def train_purchases_txt(self):\n",
    "        return self.master_data.purchases_txt[self.master_data.purchases_txt.userid.isin(self.train_users.userid)].copy(\n",
    "            deep=True)\n",
    "\n",
    "    def test_purchases_txt(self):\n",
    "        return self.master_data.purchases_txt[self.master_data.purchases_txt.userid.isin(self.test_users.userid)].copy(\n",
    "            deep=True)\n",
    "\n",
    "    def validation_purchases_txt(self):\n",
    "        return self.master_data.purchases_txt[\n",
    "            self.master_data.purchases_txt.userid.isin(self.validation_users.userid)].copy(deep=True)\n",
    "\n",
    "\n",
    "class SplitGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    TF data generator.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_df,\n",
    "            itemizer,\n",
    "            batch_size=128,\n",
    "            random_batching=True,\n",
    "            prevent_identity=False,\n",
    "            full_data=True,\n",
    "            p50_splits=True,\n",
    "            p2575_splits=False,\n",
    "            p7525_splits=False,\n",
    "            p2525_splits=False,\n",
    "            p7575_splits=False):\n",
    "\n",
    "        now = time()\n",
    "        self.prevent_identity = prevent_identity\n",
    "        self.full_data = full_data\n",
    "        self.p50_splits = p50_splits\n",
    "        self.p2575_splits = p2575_splits\n",
    "        self.p7525_splits = p7525_splits\n",
    "        self.p2525_splits = p2525_splits\n",
    "        self.p7575_splits = p7575_splits\n",
    "        self.toki = itemizer\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data_df\n",
    "        # self.data_np = self.data.to_numpy()\n",
    "\n",
    "        self.length = len(self.data)\n",
    "        self.random_batching = random_batching\n",
    "        self.on_epoch_end()\n",
    "        print(\"SplitGenerator init done in\", time() - now, \"secs.\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.length / self.batch_size)) - 1\n",
    "\n",
    "    def __call__(self, batch_size):\n",
    "        \"\"\"Allows to use the size of batch when calling the training.\"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        return self\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.random_batching:\n",
    "            self.data = self.data.sample(frac=1)\n",
    "\n",
    "        self.data['temp_itemids_p'] = self.data['itemids'].apply(shufflestr)\n",
    "        self.data['temp_itemids_p1_50'] = self.data['temp_itemids_p'].apply(split1_50)\n",
    "        self.data['temp_itemids_p2_50'] = self.data['temp_itemids_p'].apply(split2_50)\n",
    "        self.data['temp_itemids_p_25'] = self.data['temp_itemids_p'].apply(split25)\n",
    "        self.data['temp_itemids_p_75'] = self.data['temp_itemids_p'].apply(split75)\n",
    "        self.data_np = self.data.to_numpy()\n",
    "\n",
    "    def get_basket_np(self, items):\n",
    "        if self.n_ratings:\n",
    "            return np.vstack([self.embeddings_dict.get(x, self.null_val) for x in items.split(',')])\n",
    "\n",
    "        return np.vstack([self.embeddings_dict.get(x, self.null_val) for x in set(items.split(','))])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # binary mode = output vectors is 0/1 only\n",
    "        mod = 'binary'\n",
    "\n",
    "        data_slice = self.data_np[self.batch_size * index:self.batch_size * index + self.batch_size]\n",
    "\n",
    "        indices = list(range(self.__len__()))\n",
    "        indices += indices\n",
    "\n",
    "        index2 = indices[index + 1]\n",
    "        index3 = indices[index + 2]\n",
    "        index4 = indices[index + 3]\n",
    "        index5 = indices[index + 4]\n",
    "\n",
    "        if self.full_data:\n",
    "            data_slice = self.data_np[self.batch_size * index:self.batch_size * index + self.batch_size]\n",
    "\n",
    "        if self.p50_splits:\n",
    "            data_slice2 = self.data_np[self.batch_size * index2:self.batch_size * index2 + self.batch_size]\n",
    "            data_slice3 = self.data_np[self.batch_size * index3:self.batch_size * index3 + self.batch_size]\n",
    "\n",
    "        if self.p2575_splits or self.p7525_splits or self.p2525_splits or self.p7575_splits:\n",
    "            data_slice4 = self.data_np[self.batch_size * index4:self.batch_size * index4 + self.batch_size]\n",
    "            data_slice5 = self.data_np[self.batch_size * index5:self.batch_size * index5 + self.batch_size]\n",
    "\n",
    "        ret_x = []\n",
    "        ret_y = []\n",
    "\n",
    "        # full input to full_output\n",
    "        if self.full_data:\n",
    "            ret_x.append(self.toki.texts_to_matrix(data_slice[:, 1], mode=mod))\n",
    "            ret_y.append(self.toki.texts_to_matrix(data_slice[:, 1], mode=mod))\n",
    "\n",
    "        if self.p50_splits:\n",
    "            ret_x.append(self.toki.texts_to_matrix(data_slice2[:, 3], mode=mod))\n",
    "            ret_x.append(self.toki.texts_to_matrix(data_slice3[:, 4], mode=mod))\n",
    "\n",
    "            if self.prevent_identity:\n",
    "                ret_y.append(self.toki.texts_to_matrix(data_slice2[:, 4], mode=mod))\n",
    "                ret_y.append(self.toki.texts_to_matrix(data_slice3[:, 3], mode=mod))\n",
    "            else:\n",
    "                ret_y.append(self.toki.texts_to_matrix(data_slice2[:, 3], mode=mod))\n",
    "                ret_y.append(self.toki.texts_to_matrix(data_slice3[:, 4], mode=mod))\n",
    "\n",
    "        if self.p2575_splits:\n",
    "            ret_x.append(self.toki.texts_to_matrix(data_slice4[:, 5], mode=mod))\n",
    "            ret_y.append(self.toki.texts_to_matrix(data_slice4[:, 6], mode=mod))\n",
    "\n",
    "        if self.p7525_splits:\n",
    "            ret_x.append(self.toki.texts_to_matrix(data_slice4[:, 6], mode=mod))\n",
    "            ret_y.append(self.toki.texts_to_matrix(data_slice4[:, 5], mode=mod))\n",
    "\n",
    "        if self.p2525_splits:\n",
    "            ret_x.append(self.toki.texts_to_matrix(data_slice5[:, 5], mode=mod))\n",
    "            ret_y.append(self.toki.texts_to_matrix(data_slice5[:, 5], mode=mod))\n",
    "\n",
    "        if self.p7575_splits:\n",
    "            ret_x.append(self.toki.texts_to_matrix(data_slice5[:, 6], mode=mod))\n",
    "            ret_y.append(self.toki.texts_to_matrix(data_slice5[:, 6], mode=mod))\n",
    "\n",
    "        return np.vstack(ret_x), np.vstack(ret_y)\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Evaluation on give data split.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, method='leave_random_20_pct_out', data=\"test\", debug=False):\n",
    "        assert method in ['leave_random_20_pct_out', '1_20', '2_20', '3_20', '4_20', '5_20']\n",
    "        self.split = split\n",
    "        if data == \"val\":\n",
    "            print(\"Creating validation split evaluator with\", method, \"method.\")\n",
    "            self.ivx = split.validation_gen.data.set_index(split.validation_gen.data.userid).sort_index().itemids.apply(\n",
    "                lambda x: x.split(','))\n",
    "        else:\n",
    "            print(\"Creating test split evaluator with\", method, \"method.\")\n",
    "            self.ivx = split.test_gen.data.set_index(split.test_gen.data.userid).sort_index().itemids.apply(\n",
    "                lambda x: x.split(','))\n",
    "        if debug:\n",
    "            print(\"Stage 1 done.\")\n",
    "        self.tpx = []\n",
    "        if method == 'leave_random_20_pct_out':\n",
    "            for e in range(len(self.ivx)):\n",
    "                tech20 = []\n",
    "                num_to_add = int(len(self.ivx[e]) * 0.2)\n",
    "                if num_to_add < 1:\n",
    "                    num_to_add = 1\n",
    "                for x in range(num_to_add):\n",
    "                    random_pick_index = randrange(len(self.ivx[e]))\n",
    "                    tech20.append(self.ivx[e].pop(random_pick_index))\n",
    "                self.tpx.append(tech20)\n",
    "        else:\n",
    "            end = int(method[0])\n",
    "            start = end - 1\n",
    "            random.seed(get_seed())\n",
    "            for e in range(len(self.ivx)):\n",
    "                tech20 = []\n",
    "                interactions_len = len(self.ivx[e])\n",
    "                num_to_add = int(interactions_len * 0.2)\n",
    "                if num_to_add < 1:\n",
    "                    num_to_add = 1\n",
    "                shuffle(self.ivx[e])\n",
    "                for x in range(start * num_to_add, end * num_to_add):\n",
    "                    random_pick_index = x\n",
    "                    if random_pick_index >= len(self.ivx[e]):\n",
    "                        random_pick_index = len(self.ivx[e]) - 1\n",
    "                    tech20.append(self.ivx[e].pop(random_pick_index))\n",
    "                self.tpx.append(tech20)\n",
    "\n",
    "        if debug:\n",
    "            print(\"Stage 2 done.\")\n",
    "        self.iv = self.split.master_data.toki.texts_to_matrix(\n",
    "            [\",\".join(x) for x in self.ivx]\n",
    "        )\n",
    "        if debug:\n",
    "            print(\"Stage 3 done.\")\n",
    "        self.tp = self.split.master_data.toki.texts_to_matrix(\n",
    "            [\",\".join(x) for x in self.tpx]\n",
    "        )\n",
    "        self.tpx_set = [set(t) for t in self.tpx]\n",
    "        if debug:\n",
    "            print(\"Stage 4 done.\")\n",
    "\n",
    "    def update(self, m, chunk=1000):\n",
    "        assert len(self.iv) % chunk == 0\n",
    "        self.pr = np.vstack([m.predict(self.iv[chunk * x:chunk * (x + 1)]) for x in range(len(self.iv) // chunk)])\n",
    "        self.ppp = (1 - self.iv) * self.pr\n",
    "        self.ppp[:, 0] = 0\n",
    "\n",
    "    def get_ncdg(self, k):\n",
    "        pr = self.pr\n",
    "        ppp = self.ppp\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        topk_part = ppp[np.arange(ppp.shape[0])[:, np.newaxis], idx[:, :k]]\n",
    "        idx_part = np.argsort(-topk_part, axis=1)\n",
    "        idx_topk = idx[np.arange(self.iv.shape[0])[:, np.newaxis], idx_part]\n",
    "        tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "        z = zip([[self.split.master_data.toki.index_word[b] for b in a] for a in idx_topk], self.tpx_set)\n",
    "        n = np.array([(np.array([1 if x in true else 0 for x in pred]) * tp).sum() for pred, true in z])\n",
    "        d = np.array([(np.ones(min(k, len(x))) * tp[:len(x)]).sum() for x in self.tpx_set])\n",
    "        return (n / d).mean()\n",
    "\n",
    "    def get_hr(self, k):\n",
    "        pr = self.pr\n",
    "        ppp = self.ppp\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        z = zip([set([self.split.master_data.toki.index_word[s] for s in a[:k]]) for a in idx], self.tpx_set)\n",
    "        r = [0 if len(pred & true) / min(k, len(true)) == 0 else 1 for pred, true in z]\n",
    "        return sum(r) / len(r)\n",
    "\n",
    "    def get_coverage(self, k):\n",
    "        pr = self.pr\n",
    "        ppp = self.ppp\n",
    "\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        covered = len(set().union(*[set([self.split.master_data.toki.index_word[s] for s in a[:k]]) for a in idx]))\n",
    "        total = self.iv.shape[1] - 1\n",
    "        return covered / total\n",
    "\n",
    "    def get_recall(self, k):\n",
    "        pr = self.pr\n",
    "        ppp = self.ppp\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        z = zip([set([self.split.master_data.toki.index_word[s] for s in a[:k]]) for a in idx], self.tpx_set)\n",
    "        r = [len(pred & true) / min(k, len(true)) for pred, true in z]\n",
    "        return sum(r) / len(r)\n",
    "\n",
    "    def ncdg(self, m, k):\n",
    "        c = getattr(m, \"pred_from_mean\", None)\n",
    "        if not callable(c):\n",
    "            c = getattr(m, \"predict\", None)\n",
    "        pr = c(self.iv)\n",
    "        ppp = (1 - self.iv) * pr\n",
    "        ppp[:, 0] = 0\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        topk_part = ppp[np.arange(ppp.shape[0])[:, np.newaxis], idx[:, :k]]\n",
    "        idx_part = np.argsort(-topk_part, axis=1)\n",
    "        idx_topk = idx[np.arange(self.iv.shape[0])[:, np.newaxis], idx_part]\n",
    "        tp = 1. / np.log2(np.arange(2, k + 2))\n",
    "        z = zip([[self.split.master_data.toki.index_word[b] for b in a] for a in idx_topk], self.tpx_set)\n",
    "        n = np.array([(np.array([1 if x in true else 0 for x in pred]) * tp).sum() for pred, true in z])\n",
    "        d = np.array([(np.ones(min(k, len(x))) * tp[:len(x)]).sum() for x in self.tpx_set])\n",
    "        return (n / d).mean()\n",
    "\n",
    "    def hr(self, m, k):\n",
    "        c = getattr(m, \"pred_from_mean\", None)\n",
    "        if not callable(c):\n",
    "            c = getattr(m, \"predict\", None)\n",
    "        pr = c(self.iv)\n",
    "        ppp = (1 - self.iv) * pr\n",
    "        ppp[:, 0] = 0\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        z = zip([set([self.split.master_data.toki.index_word[s] for s in a[:k]]) for a in idx], self.tpx_set)\n",
    "        r = [0 if len(pred & true) / min(k, len(true)) == 0 else 1 for pred, true in z]\n",
    "        return sum(r) / len(r)\n",
    "\n",
    "    def coverage(self, m, k):\n",
    "        c = getattr(m, \"pred_from_mean\", None)\n",
    "        if not callable(c):\n",
    "            c = getattr(m, \"predict\", None)\n",
    "        pr = c(self.iv)\n",
    "        ppp = (1 - self.iv) * pr\n",
    "        ppp[:, 0] = 0\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        covered = len(set().union(*[set([self.split.master_data.toki.index_word[s] for s in a[:k]]) for a in idx]))\n",
    "        total = self.iv.shape[1] - 1\n",
    "        return covered / total\n",
    "\n",
    "    def recall(self, m, k):\n",
    "        c = getattr(m, \"pred_from_mean\", None)\n",
    "        if not callable(c):\n",
    "            c = getattr(m, \"predict\", None)\n",
    "        pr = c(self.iv)\n",
    "        ppp = (1 - self.iv) * pr\n",
    "        ppp[:, 0] = 0\n",
    "        idx = bn.argpartition(-ppp, k, axis=1)\n",
    "        z = zip([set([self.split.master_data.toki.index_word[s] for s in a[:k]]) for a in idx], self.tpx_set)\n",
    "        r = [len(pred & true) / min(k, len(true)) for pred, true in z]\n",
    "        return sum(r) / len(r)\n",
    "\n",
    "\n",
    "# Model abstract class\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    Abstract model.\n",
    "    Subclassed model should implements create_model and train_model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, name):\n",
    "        self.split = split\n",
    "        self.dataset = split.master_data\n",
    "        self.metrics = {\n",
    "            'Recall@5': {'k': 5, 'method': self.split.evaluator.get_recall, 'value': None},\n",
    "            'Recall@20': {'k': 20, 'method': self.split.evaluator.get_recall, 'value': None},\n",
    "            'Recall@50': {'k': 50, 'method': self.split.evaluator.get_recall, 'value': None},\n",
    "            'NCDG@100': {'k': 100, 'method': self.split.evaluator.get_ncdg, 'value': None},\n",
    "            'Coverage@5': {'k': 5, 'method': self.split.evaluator.get_coverage, 'value': None},\n",
    "            'Coverage@20': {'k': 20, 'method': self.split.evaluator.get_coverage, 'value': None},\n",
    "            'Coverage@50': {'k': 50, 'method': self.split.evaluator.get_coverage, 'value': None},\n",
    "            'Coverage@100': {'k': 100, 'method': self.split.evaluator.get_coverage, 'value': None},\n",
    "        }\n",
    "        self.name = name\n",
    "\n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Build Your own model here\n",
    "        \"\"\"\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Create your own training loop here\n",
    "        \"\"\"\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        self.split.evaluator.update(self.model)\n",
    "        for x in self.metrics.values():\n",
    "            x['value'] = x['method'](x['k'])\n",
    "\n",
    "    def print_metrics(self):\n",
    "        print(\"Model metrics:\", end='')\n",
    "        for k, x in self.metrics.items():\n",
    "            print(k, end=\"=\")\n",
    "            print(round(x['value'], 4), end=\" \")\n",
    "        print()\n",
    "\n",
    "    def test_model(self):\n",
    "        e = self.split.test_evaluator\n",
    "        e.update(self.model)\n",
    "        print(\"Results for test set: Recall@20=\", e.get_recall(20), \", Recall@50=\", e.get_recall(50), \", NCDG@100=\",\n",
    "              e.get_ncdg(100), sep=\"\")\n",
    "        with open(\"seed_results_test.txt\", \"a\") as myfile:\n",
    "            myfile.write(\"Results for test set: Recall@20=\" + str(e.get_recall(20)) + \", Recall@50=\" + str(\n",
    "                e.get_recall(50)) + \", NCDG@100=\" + str(e.get_ncdg(100)) + \"\\n\")\n",
    "\n",
    "    def test_model_val(self):\n",
    "        e = self.split.evaluator\n",
    "        e.update(self.model)\n",
    "        print(\"Results for validation set: Recall@20=\", e.get_recall(20), \", Recall@50=\", e.get_recall(50),\n",
    "              \", NCDG@100=\",\n",
    "              e.get_ncdg(100), sep=\"\")\n",
    "        with open(\"seed_results_val.txt\", \"a\") as myfile:\n",
    "            myfile.write(\"Results for validation set: Recall@20=\" + str(e.get_recall(20)) + \", Recall@50=\" + str(\n",
    "                e.get_recall(50)) + \", NCDG@100=\" + str(e.get_ncdg(100)) + \"\\n\")\n",
    "\n",
    "\n",
    "# Tensorflow objects - Callbacks\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Evaluate model in tf callback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rsmodel):\n",
    "        super(MetricsCallback, self).__init__()\n",
    "        self.epoch = 0\n",
    "        self.loss_metrics = dict()\n",
    "        self.eval_metrics = dict()\n",
    "        self.evaluate_loss_metrics = ['loss', 'val_loss']\n",
    "        self.rsmodel = rsmodel\n",
    "        self.best_ncdg100 = 0.\n",
    "        self.best_ncdg100_epoch = 0\n",
    "        self.best_recall20 = 0.\n",
    "        self.best_recall20_epoch = 0\n",
    "        self.best_recall50 = 0.\n",
    "        self.best_recall50_epoch = 0\n",
    "        self.tsne_df = pd.DataFrame(columns=[\"epoch\", \"tsne_coords\"])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch += 1\n",
    "\n",
    "        self.loss_metrics[self.epoch] = dict()\n",
    "        self.eval_metrics[self.epoch] = dict()\n",
    "        # add metrics from logs\n",
    "        for x in self.evaluate_loss_metrics:\n",
    "            self.loss_metrics[self.epoch][x] = logs[x]\n",
    "        # add custom metrics\n",
    "        self.rsmodel.evaluate_model()\n",
    "        self.rsmodel.print_metrics()\n",
    "        for x in self.rsmodel.metrics.keys():\n",
    "            self.eval_metrics[self.epoch][x] = self.rsmodel.metrics[x]['value']\n",
    "        self.ncdg_100_watch()\n",
    "        self.recall20_watch()\n",
    "        self.recall50_watch()\n",
    "        # self.get_history_df()\n",
    "        # self.calc_tsne()\n",
    "\n",
    "    def recall20_watch(self):\n",
    "        if self.eval_metrics[self.epoch]['Recall@20'] > self.best_recall20:\n",
    "            print(\"New best for Recall@20\")\n",
    "            self.model.save_weights(self.rsmodel.name + \"_best_recall_20/\" + self.rsmodel.name)\n",
    "            self.best_recall20 = self.eval_metrics[self.epoch]['Recall@20']\n",
    "            self.best_recall20_epoch = self.epoch\n",
    "            with open(self.rsmodel.name + \"_best_recall_20/\" + \"epoch.txt\", \"w\") as text_file:\n",
    "                text_file.write(str(self.best_recall20_epoch))\n",
    "\n",
    "    def recall50_watch(self):\n",
    "        if self.eval_metrics[self.epoch]['Recall@50'] > self.best_recall50:\n",
    "            print(\"New best for Recall@50\")\n",
    "            self.model.save_weights(self.rsmodel.name + \"_best_recall_50/\" + self.rsmodel.name)\n",
    "            self.best_recall50 = self.eval_metrics[self.epoch]['Recall@50']\n",
    "            self.best_recall50_epoch = self.epoch\n",
    "            with open(self.rsmodel.name + \"_best_recall_50/\" + \"epoch.txt\", \"w\") as text_file:\n",
    "                text_file.write(str(self.best_recall50_epoch))\n",
    "\n",
    "    def ncdg_100_watch(self):\n",
    "        if self.eval_metrics[self.epoch]['NCDG@100'] > self.best_ncdg100:\n",
    "            print(\"New best for NCDG@100\")\n",
    "            self.model.save_weights(self.rsmodel.name + \"_best_ncdg_100/\" + self.rsmodel.name)\n",
    "            self.best_ncdg100 = self.eval_metrics[self.epoch]['NCDG@100']\n",
    "            self.best_ncdg100_epoch = self.epoch\n",
    "            with open(self.rsmodel.name + \"_best_ncdg_100/\" + \"epoch.txt\", \"w\") as text_file:\n",
    "                text_file.write(str(self.best_ncdg100_epoch))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        self.plot_history()\n",
    "\n",
    "    def get_history_df(self):\n",
    "\n",
    "        outt1 = {\n",
    "            'epochs': [x for x in self.rsmodel.mc.loss_metrics.keys()]\n",
    "        }\n",
    "\n",
    "        outt2 = {\n",
    "            'epochs': [x for x in self.rsmodel.mc.eval_metrics.keys()]\n",
    "        }\n",
    "\n",
    "        for k in self.loss_metrics[1].keys():\n",
    "            outt1[k] = [self.loss_metrics[x][k] for x in self.loss_metrics.keys()]\n",
    "\n",
    "        for k in self.eval_metrics[1].keys():\n",
    "            outt2[k] = [self.eval_metrics[x][k] for x in self.eval_metrics.keys()]\n",
    "\n",
    "        self.history_loss_df = pd.DataFrame(outt1)\n",
    "        self.history_loss_df.to_json(self.rsmodel.name + \"_loss.json\")\n",
    "\n",
    "        self.history_df = pd.DataFrame(outt2)\n",
    "        self.history_df.to_json(self.rsmodel.name + \"_metrics.json\")\n",
    "\n",
    "        return self.history_df\n",
    "\n",
    "    def plot_history(self):\n",
    "        return self.get_history_df().set_index(self.history_df.epochs, drop=True).iloc[:, 1:].plot(figsize=(20, 10))\n",
    "\n",
    "    def calc_tsne(self):\n",
    "        num_words = self.rsmodel.dataset.num_words\n",
    "        input_single_item_matrix = np.zeros((num_words, num_words))\n",
    "        np.fill_diagonal(input_single_item_matrix, 1.)\n",
    "        qqq = scale_d(self.model.predict(input_single_item_matrix)).numpy() * .99\n",
    "        np.fill_diagonal(qqq, 1.)\n",
    "        tsne_coordinates = TSNE(n_components=2, metric=\"precomputed\", angle=0.5, perplexity=30, random_state=6).fit(\n",
    "            (1 - qqq))\n",
    "        tsne_coordinates = tsne_coordinates.embedding_\n",
    "        self.tsne_df.loc[self.epoch] = [self.epoch, tsne_coordinates]\n",
    "        self.tsne_df.to_json(self.rsmodel.name + \"_tsne.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading users_pu5\n",
      "Reading items_pu5\n",
      "Reading purchases_txt_pu5\n",
      "Reading items_sorted_pu5\n",
      "Reading users_sorted_pu5\n",
      "Read all in 0.10833907127380371\n",
      "Tokenizer trained for 3534 items.\n",
      "Creating 1 splits of 10000 samples each.\n",
      "Creating split nr. 1\n"
     ]
    }
   ],
   "source": [
    "dataset = Data(d='../data/ml-1m/processed/', pruning='u5')\n",
    "dataset.splits = []\n",
    "dataset.create_splits(1, 10000, shuffle=False, generators=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.split.train_users = pd.read_json(\"../data/ml-1m/processed/train_users.json\").userid.apply(str).to_frame()\n",
    "dataset.split.validation_users = pd.read_json(\"../data/ml-1m/processed/val_users.json\").userid.apply(str).to_frame()\n",
    "dataset.split.test_users = pd.read_json(\"../data/ml-1m/processed/test_users.json\").userid.apply(str).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Split' object has no attribute 'evaluator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000012?line=0'>1</a>\u001b[0m m \u001b[39m=\u001b[39m VASP(dataset\u001b[39m.\u001b[39;49msplit, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mVASP_ML20_1\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000012?line=1'>2</a>\u001b[0m m\u001b[39m.\u001b[39mcreate_model(latent\u001b[39m=\u001b[39m\u001b[39m2048\u001b[39m, hidden\u001b[39m=\u001b[39m\u001b[39m4096\u001b[39m, ease_items_sampling\u001b[39m=\u001b[39m\u001b[39m0.33\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000012?line=2'>3</a>\u001b[0m m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msummary()\n",
      "\u001b[1;32m/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb Cell 7'\u001b[0m in \u001b[0;36mModel.__init__\u001b[0;34m(self, split, name)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000006?line=8'>9</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit \u001b[39m=\u001b[39m split\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000006?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m split\u001b[39m.\u001b[39mmaster_data\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000006?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics \u001b[39m=\u001b[39m {\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000006?line=11'>12</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mRecall@5\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mk\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m5\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit\u001b[39m.\u001b[39;49mevaluator\u001b[39m.\u001b[39mget_recall, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000006?line=12'>13</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mRecall@20\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mk\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m20\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit\u001b[39m.\u001b[39mevaluator\u001b[39m.\u001b[39mget_recall, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000006?line=13'>14</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mRecall@50\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mk\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m50\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit\u001b[39m.\u001b[39mevaluator\u001b[39m.\u001b[39mget_recall, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000006?line=14'>15</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mNCDG@100\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mk\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit\u001b[39m.\u001b[39mevaluator\u001b[39m.\u001b[39mget_ncdg, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000006?line=15'>16</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mCoverage@5\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mk\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m5\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit\u001b[39m.\u001b[39mevaluator\u001b[39m.\u001b[39mget_coverage, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000006?line=16'>17</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mCoverage@20\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mk\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m20\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit\u001b[39m.\u001b[39mevaluator\u001b[39m.\u001b[39mget_coverage, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000006?line=17'>18</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mCoverage@50\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mk\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m50\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit\u001b[39m.\u001b[39mevaluator\u001b[39m.\u001b[39mget_coverage, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000006?line=18'>19</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mCoverage@100\u001b[39m\u001b[39m'\u001b[39m: {\u001b[39m'\u001b[39m\u001b[39mk\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmethod\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit\u001b[39m.\u001b[39mevaluator\u001b[39m.\u001b[39mget_coverage, \u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mNone\u001b[39;00m},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000006?line=19'>20</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abharani/PycharmProjects/movielens-recommendation-engine/VASP/utils.ipynb#ch0000006?line=20'>21</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m name\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Split' object has no attribute 'evaluator'"
     ]
    }
   ],
   "source": [
    "m = VASP(dataset.split, name=\"VASP_ML20_1\")\n",
    "m.create_model(latent=2048, hidden=4096, ease_items_sampling=0.33)\n",
    "m.model.summary()\n",
    "print(\"=\" * 80)\n",
    "print(\"Train for 50 epochs with lr 0.00005\")\n",
    "m.compile_model(lr=0.00005, fl_alpha=0.25, fl_gamma=2.0)\n",
    "m.train_model(50)\n",
    "print(\"=\" * 80)\n",
    "print(\"Than train for 20 epochs with lr 0.00001\")\n",
    "m.compile_model(lr=0.00001, fl_alpha=0.25, fl_gamma=2.0)\n",
    "m.train_model(20)\n",
    "print(\"=\" * 80)\n",
    "print(\"Than train for 20 epochs with lr 0.000001\")\n",
    "m.compile_model(lr=0.00001, fl_alpha=0.25, fl_gamma=2.0)\n",
    "m.train_model(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c3aa81f53feaf22594607225e353cd7b61927a2847d5a10e1d40f929f7c9ceb6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('airflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
